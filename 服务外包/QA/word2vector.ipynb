{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  4.43269173e-03   6.23548729e-03   6.90771267e-05   2.13220075e-04\n",
      "   5.47724916e-03   5.70030883e-03  -5.10864658e-03   7.82835297e-03\n",
      "  -3.09145427e-04  -7.66132958e-03  -5.96775534e-03   5.27057424e-03\n",
      "  -6.25812961e-03   3.05820815e-03   7.91353697e-04   1.49712921e-03\n",
      "   4.94984817e-03  -3.58482264e-03   1.05283465e-02  -4.59967088e-03\n",
      "  -3.79278930e-03   8.51860177e-03  -8.74970108e-03  -1.49513246e-03\n",
      "   5.48866112e-03  -6.51512062e-03  -1.70180434e-03  -1.91129162e-04\n",
      "   3.13324155e-03  -2.17564823e-03  -7.33267469e-03   4.22674511e-03\n",
      "  -4.66940645e-03   2.98876246e-03   8.15330632e-03   2.95639457e-03\n",
      "   3.45487986e-03   5.89327607e-03   4.66167741e-03   4.47103847e-03\n",
      "  -1.17158070e-02  -6.55473676e-03   7.41658325e-04  -5.32908714e-04\n",
      "  -2.36614118e-03   4.61190240e-03  -1.82505208e-03   2.96918489e-03\n",
      "   1.07366964e-03  -7.64057878e-03   2.43307464e-03  -5.04201511e-04\n",
      "  -1.56730204e-03   2.33003451e-03  -1.08037755e-04   2.60429992e-03\n",
      "  -8.12250562e-03  -6.79801311e-03  -7.69185880e-03  -6.96645491e-03\n",
      "   5.49208233e-03  -5.33887185e-03  -4.79274429e-03   8.97895917e-03\n",
      "  -3.24553018e-03  -4.84137284e-03   4.11092443e-03   5.13997674e-03\n",
      "  -6.43836381e-03  -1.61758084e-02  -1.06231775e-02   1.33815687e-03\n",
      "   7.57883210e-03   1.23535097e-03   1.26494765e-02   3.59503273e-03\n",
      "   3.15536582e-03   1.45665149e-03  -6.18426362e-03   1.08281651e-03\n",
      "   3.60591011e-03  -8.31536308e-04   4.79938899e-04  -7.98358466e-04\n",
      "   1.68482750e-03  -6.89196866e-03   4.97955456e-03   2.84626102e-03\n",
      "   1.46252301e-03  -2.65785540e-03  -3.16599989e-03   6.84677158e-03\n",
      "   4.92369011e-03  -1.30557932e-03   6.64091622e-03  -6.75333198e-04\n",
      "  -3.36205028e-03   8.83750501e-04  -4.83000278e-03   3.82109894e-03]\n"
     ]
    }
   ],
   "source": [
    "import xlrd\n",
    "import xlwt\n",
    "from random import choice\n",
    "from gensim.models import word2vec\n",
    "from gensim.models.word2vec import Word2Vec \n",
    "import jieba\n",
    "import jieba.analyse\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Conv1D, MaxPooling1D, Dropout, Input, concatenate\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Model\n",
    "import os\n",
    "import numpy as np\n",
    "####################################################################\n",
    "#\n",
    "#using word2vec to train a word_vectors\n",
    "#\n",
    "####################################################################\n",
    "def read_excel(path):\n",
    "    excel=xlrd.open_workbook(path)\n",
    "    sheet1=excel.sheet_by_index(0)\n",
    "    col_1=sheet1.col_values(0)\n",
    "    col_2=sheet1.col_values(1)\n",
    "    col_3=sheet1.col_values(2)\n",
    "    return col_1,col_2,col_3\n",
    "def stopwordslist(filepath):  \n",
    "    stopwords = [line.strip() for line in open(filepath, 'r', encoding='utf-8').readlines()]  \n",
    "    return stopwords\n",
    "\n",
    "def write_input():\n",
    "    path=\"../data/train.xls\"\n",
    "    c1,c2,c3=read_excel(path)\n",
    "    inputs = open(\"../data/input.txt\", 'w', encoding='utf-8') \n",
    "    '''for i in c1:\n",
    "        if((str)(i)!=\"训练问题\"):\n",
    "            inputs.write(i+\"\\n\")\n",
    "    for i in c2:\n",
    "        if((str)(i)!=\"正向答案\"):\n",
    "            inputs.write(i+\"\\n\")\n",
    "            '''\n",
    "    for i in c3:\n",
    "        if((str)(i)!=\"负向答案\"):\n",
    "            inputs.write(i+\"\\n\")\n",
    "\n",
    "    inputs.close()\n",
    "# split sentences by stopwords \n",
    "def seg_sentence(sentence):  \n",
    "    sentence_seged = jieba.cut(sentence.strip(),cut_all=False)  \n",
    "    stopwords = stopwordslist(\"../data/stopwords.txt\") \n",
    "    outstr = ''  \n",
    "    for word in sentence_seged:  \n",
    "        if word not in stopwords:  \n",
    "            if word != '\\t':  \n",
    "                outstr += word  \n",
    "                outstr += \" \"  \n",
    "    return outstr.encode('utf-8')  \n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    #get input txt\n",
    "    '''\n",
    "    write_input()\n",
    "    inputs = open(\"../data/input.txt\", 'r', encoding='utf-8') \n",
    "    outputs = open(\"../data/output.txt\", 'w',encoding='utf-8') \n",
    "    for line in inputs:  \n",
    "        line_seg = seg_sentence(line)\n",
    "        outputs.write(str(line_seg.decode('utf-8').strip()+\"\\n\"))\n",
    "    #print(\"done\")\n",
    "    outputs.close()  \n",
    "    inputs.close()\n",
    "    '''\n",
    "    sentences = word2vec.Text8Corpus(\"../data/output_all.txt\")\n",
    "    model = word2vec.Word2Vec(sentences, hs=1,min_count=1,window=3,size=100)\n",
    "    model.wv.save_word2vec_format(\"../data/vectors.txt\",binary=False)\n",
    "    print(model[\"0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
