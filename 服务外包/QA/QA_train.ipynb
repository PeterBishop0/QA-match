{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os, time, datetime, operator, sys\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from operator import itemgetter\n",
    "sys.path.append('../')\n",
    "v_trainfile = '../data/output_all.txt'\n",
    "trainfile_1='../data/output_1.txt'\n",
    "trainfile_2='../data/output_2.txt'\n",
    "trainfile_3='../data/output_3.txt'\n",
    "#vectorsfile = '../data/vectors.txt'\n",
    "##########################################################################\n",
    "#  embedding_lookup + cnn + cosine margine ,  batch\n",
    "##########################################################################\n",
    "class QACNN(object):\n",
    "    def __init__(self, _margin, sequence_length, batch_size,vocab_size, embedding_size,filter_sizes, num_filters, l2_reg_lambda=0.0):\n",
    "        self.L, self.B, self.V, self.E, self.FS, self.NF = sequence_length, batch_size,vocab_size, embedding_size, filter_sizes, num_filters \n",
    "\n",
    "        #用户问题,字向量使用embedding_lookup\n",
    "        self.q = tf.placeholder(tf.int32, [self.B, self.L], name=\"q\")\n",
    "        #待匹配正向问题\n",
    "        self.qp = tf.placeholder(tf.int32, [self.B, self.L], name=\"qp\")\n",
    "        #负向问题\n",
    "        self.qn = tf.placeholder(tf.int32, [self.B, self.L], name=\"qn\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "        l2_loss = tf.constant(0.0)\n",
    "\n",
    "        # Embedding layer\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            W = tf.get_variable(initializer=tf.random_uniform([self.V, self.E], -1.0, 1.0),name='We')\n",
    "            self.qe = tf.nn.embedding_lookup(W, self.q)\n",
    "            self.qpe = tf.nn.embedding_lookup(W, self.qp)\n",
    "            self.qne = tf.nn.embedding_lookup(W, self.qn)\n",
    "        self.qe = tf.expand_dims(self.qe, -1)\n",
    "        self.qpe = tf.expand_dims(self.qpe, -1)\n",
    "        self.qne = tf.expand_dims(self.qne, -1)\n",
    "        with tf.variable_scope('shared-conv') as scope:\n",
    "            self.qe = self.conv(self.qe)\n",
    "            scope.reuse_variables()\n",
    "            #tf.get_variable_scope().reuse_variables()\n",
    "            self.qpe = self.conv(self.qpe)\n",
    "            scope.reuse_variables()\n",
    "            #tf.get_variable_scope().reuse_variables()\n",
    "            self.qne = self.conv(self.qne)\n",
    "        self.cos_q_qp = self.cosine(self.qe, self.qpe)\n",
    "        self.cos_q_qn = self.cosine(self.qe, self.qne)\n",
    "        zero = tf.constant(0, shape=[self.B], dtype=tf.float32)\n",
    "        margin = tf.constant(_margin, shape=[self.B], dtype=tf.float32)\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            self.losses = tf.maximum(zero, tf.subtract(margin, tf.subtract(self.cos_q_qp, self.cos_q_qn)))\n",
    "            self.loss = tf.reduce_sum(self.losses) + l2_reg_lambda * l2_loss\n",
    "            print('loss ', self.loss)\n",
    "\n",
    "        # Accuracy\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            self.correct = tf.equal(zero, self.losses)\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(self.correct, \"float\"), name=\"accuracy\")\n",
    "\n",
    "        for v in tf.trainable_variables():\n",
    "            print(v)\n",
    "\n",
    "    def conv(self, tensor):\n",
    "        pooled = []   \n",
    "        with tf.variable_scope(\"my-conv-shared\"):\n",
    "            for i, fs in enumerate(self.FS):\n",
    "                filter_shape = [fs, self.E, 1, self.NF]\n",
    "                W = tf.get_variable(initializer=tf.truncated_normal(filter_shape, stddev=0.1), name=\"W-%s\" % str(fs))\n",
    "                b = tf.get_variable(initializer=tf.constant(0.1, shape=[self.NF]), name=\"b-%s\" % str(fs))\n",
    "                conv = tf.nn.conv2d(tensor, W, strides=[1, 1, 1, 1], padding='VALID',name=\"conv\")\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "                output = tf.nn.max_pool(h, ksize=[1, self.L - fs + 1, 1, 1], strides=[1, 1, 1, 1], padding='VALID',name=\"pool\")\n",
    "                pooled.append(output)\n",
    "            num_filters_total = self.NF * len(self.FS)\n",
    "            pooled = tf.reshape(tf.concat(pooled, 3), [-1, num_filters_total])\n",
    "            pooled = tf.nn.dropout(pooled, self.dropout_keep_prob)\n",
    "            return pooled\n",
    "\n",
    "    def cosine(self, v1, v2):\n",
    "        l1 = tf.sqrt(tf.reduce_sum(tf.multiply(v1, v1), 1))\n",
    "        l2 = tf.sqrt(tf.reduce_sum(tf.multiply(v2, v2), 1))\n",
    "        a = tf.reduce_sum(tf.multiply(v1, v2), 1)\n",
    "        cos = tf.div(a, tf.multiply(l1, l2), name='score')\n",
    "        return tf.clip_by_value(cos, 1e-5, 0.99999)\n",
    "\n",
    "#build a vocabulary list based on splited txt\n",
    "def build_vocab():\n",
    "    global v_trainfile\n",
    "    code, vocab = int(0), {}\n",
    "    vocab['UNKNOWN'] = code\n",
    "    code += 1\n",
    "    with open(v_trainfile,encoding='utf-8') as f:\n",
    "        lines=f.readlines()\n",
    "    for line in lines:\n",
    "        items = line.encode('utf-8').decode('utf-8-sig').strip().split()\n",
    "        for i in range(0, len(items)):\n",
    "            word=(str)(items[i])\n",
    "            if not word in vocab and word!='':\n",
    "                vocab[word] = code\n",
    "                code += 1\n",
    "    return vocab\n",
    "'''\n",
    "#if not using tensorflow to train w2v model\n",
    "#load trained w2v vectors\n",
    "def load_vectors():\n",
    "    global vectorsfile\n",
    "    vectors = {}\n",
    "    with open(vectorsfile,encoding='utf-8') as f:\n",
    "        contents=f.read().encode('utf-8').decode('utf-8-sig').split()\n",
    "    for i in range(0,int(len(contents)/101)):\n",
    "        vec = []\n",
    "        \n",
    "        for j in range(1,101):\n",
    "            vec.append(float(contents[j+i*101]))\n",
    "        vectors[contents[i*101]]=vec\n",
    "    return vectors\n",
    "#match words' vector with vocab (brute force) \n",
    "def load_word_embeddings(vocab, dim):\n",
    "    vectors = load_vectors()\n",
    "    embeddings = [] #brute initialization\n",
    "    for i in range(0, len(vocab)):\n",
    "        vec = []\n",
    "        for j in range(0, dim):\n",
    "            vec.append(0.01)\n",
    "        embeddings.append(vec)\n",
    "    for word, code in vocab.items():\n",
    "        if word in vectors:\n",
    "            embeddings[code] = vectors[word]\n",
    "    return np.array(embeddings, dtype='float32')\n",
    "'''\n",
    "#read txts pay attention to encoding\n",
    "def read_txt(path):\n",
    "    x=[]\n",
    "    with open(path,'r',encoding='utf-8') as f:\n",
    "        lines=f.readlines()\n",
    "    for line in lines:\n",
    "        x.append(line.encode('utf-8').decode('utf-8-sig').strip())\n",
    "    return x\n",
    "#load trainList as [q, s_pq, s_nq, id]\n",
    "def load_train_list():\n",
    "    global trainfile_1\n",
    "    global trainfile_2\n",
    "    global trainfile_3\n",
    "    c1=read_txt(trainfile_1)\n",
    "    c2=read_txt(trainfile_2)\n",
    "    c3=read_txt(trainfile_3)\n",
    "    trainList = []\n",
    "    for i in range(0,len(c1)):\n",
    "        trainList.append([c1[i]]+[c2[i]]+[c3[i]]+[i])#[q,s_pq,s_nq,id]\n",
    "    return trainList\n",
    "#load testList as [flag, test_q, ans_q, id]\n",
    "def load_test_list(trainList):\n",
    "    testList = []\n",
    "    test_id=[]\n",
    "    #randomly match pos and neg question pair\n",
    "    for i in range(0,50):\n",
    "        test_id.append(random.randint(0, len(trainList)-1))\n",
    "        testList.append([1,trainList[test_id[i]][0],trainList[test_id[i]][1],trainList[test_id[i]][3]])\n",
    "        testList.append([0,trainList[test_id[i]][0],trainList[test_id[i]][2],trainList[test_id[i]][3]])\n",
    "        for j in range(0,18):\n",
    "            testList.append([0,trainList[test_id[i]][0],trainList[random.randint(0, len(trainList)-1)][2],trainList[test_id[i]][3]])\n",
    "        del trainList[test_id[i]]\n",
    "    return testList\n",
    "\n",
    "# encode string(splited words) as code value in vocab\n",
    "def encode_sent(vocab, string, size):\n",
    "    x = []\n",
    "    words = string.split()\n",
    "    if(len(words)<size):\n",
    "        words+=['0']*(size-len(words))\n",
    "    for i in range(0, size):\n",
    "        if words[i] in vocab:\n",
    "            x.append(vocab[words[i]])\n",
    "        else:\n",
    "            x.append(vocab['UNKNOWN'])\n",
    "    return x\n",
    "#turn elements in trainList into coded value for train \n",
    "def load_data(trainList, vocab, batch_size):\n",
    "    train_1, train_2, train_3 = [], [], []\n",
    "    for i in range(0, batch_size):\n",
    "        seq=trainList[random.randint(0, len(trainList)-1)]\n",
    "        train_1.append(encode_sent(vocab, seq[0], 100))\n",
    "        train_2.append(encode_sent(vocab, seq[1], 100))\n",
    "        train_3.append(encode_sent(vocab, seq[2], 100))\n",
    "    return np.array(train_1, dtype='float32'), np.array(train_2, dtype='float32'), np.array(train_3, dtype='float32')\n",
    "#generate a batch of test data\n",
    "def gen_test_batch_qpn(testList,start,end,vocab):\n",
    "    f,tq,aq,id=[],[],[],[]\n",
    "    if end<len(testList):\n",
    "        for i in range(start,end):\n",
    "            s=testList[i]\n",
    "            f.append(s[0])\n",
    "            tq.append(encode_sent(vocab,s[1],100))\n",
    "            aq.append(encode_sent(vocab,s[2],100))\n",
    "            id.append(s[3])\n",
    "    else:\n",
    "        for i in range(start,len(testList)):\n",
    "            s=testList[i]\n",
    "            f.append(s[0])\n",
    "            tq.append(encode_sent(vocab,s[1],100))\n",
    "            aq.append(encode_sent(vocab,s[2],100))\n",
    "            id.append(s[3])\n",
    "        for i in range(0,end-len(testList)):\n",
    "            s=testList[i]\n",
    "            f.append(s[0])\n",
    "            tq.append(encode_sent(vocab,s[1],100))\n",
    "            aq.append(encode_sent(vocab,s[2],100))\n",
    "            id.append(s[3])\n",
    "    tq=np.array(tq, dtype='float32')\n",
    "    aq=np.array(aq, dtype='float32')\n",
    "    return f,id,tq,aq\n",
    "#evaluate auc\n",
    "def eval_auc(y,yp):\n",
    "    auc = metrics.roc_auc_score(y, yp)\n",
    "    print('auc: ' + str(auc))\n",
    "    return auc\n",
    "#evaluate  top1_prec and F1 in a batch test\n",
    "def eval_top1_prec(y, g, yp):\n",
    "    _list = [(_y, _g, _yp) for _y, _g, _yp in zip(y, g, yp)]\n",
    "    _dict = {}\n",
    "    for _y, _g, _yp in _list:\n",
    "        if not _g in _dict: _dict[_g] = []\n",
    "        _dict[_g].append((_y, _g, _yp))\n",
    "    positive, gc = 0 , 0\n",
    "    P=1\n",
    "    for _, group in _dict.items():\n",
    "        group = sorted(group, key=itemgetter(2), reverse=True)\n",
    "        gc += 1\n",
    "        if group[0][0] == 1: \n",
    "            positive += 1\n",
    "    prec = positive / gc\n",
    "    R=prec\n",
    "    F1=2*R/(1+R)\n",
    "    print('top1 precision ' + str(positive) + '/' + str(gc) + ': '+ str(positive / gc))\n",
    "    print('F1: %f'%F1)\n",
    "    return prec,F1\n",
    "    \n",
    "# Parameters\n",
    "# ==================================================\n",
    "\n",
    "# Model Hyperparameters,restart after loading once\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
    "tf.flags.DEFINE_float(\"margin\", 0.05, \"CNN model margin\")\n",
    "tf.flags.DEFINE_integer(\"sequence_length\", 100, \"Max sequence lehgth(default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"embedding_dim\", 100, \"Dimensionality of character embedding (default: 128)\")\n",
    "tf.flags.DEFINE_string(\"filter_sizes\", \"1,2,3,5\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
    "tf.flags.DEFINE_integer(\"num_filters\", 256, \"Number of filters per filter size (default: 128)\")\n",
    "tf.flags.DEFINE_float(\"dropout_keep_prob\", 1.0, \"Dropout keep probability (default: 0.5)\")\n",
    "tf.flags.DEFINE_float(\"l2_reg_lambda\", 0, \"L2 regularizaion lambda (default: 0.0)\")\n",
    "# Training parameters\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 256, \"Batch Size (default: 64)\")\n",
    "tf.flags.DEFINE_integer(\"num_epochs\", 20000, \"Number of training epochs (default: 200)\")\n",
    "tf.flags.DEFINE_integer(\"evaluate_every\", 500, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"checkpoint_every\", 500, \"Save model after this many steps (default: 100)\")\n",
    "# Misc Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "#FLAGS = tf.flags.FLAGS #new veision\n",
    "FLAGS._parse_flags() #old version\n",
    "FLAGS.flag_values_dict()\n",
    "print(\"\\nParameters:\")\n",
    "for attr, value in sorted(FLAGS.__flags.items()):\n",
    "    print(\"{}={}\".format(attr.upper(), value))\n",
    "print(\"\")\n",
    "\n",
    "# Data Preparatopn\n",
    "# ==================================================\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "vocab = build_vocab()\n",
    "trainList=load_train_list()\n",
    "print(\"Load done...\")\n",
    "\n",
    "# Training\n",
    "# ==================================================\n",
    "\n",
    "#define session_conf\n",
    "with tf.Graph().as_default():\n",
    "    with tf.device(\"/gpu:1\"):\n",
    "        session_conf = tf.ConfigProto(allow_soft_placement=FLAGS.allow_soft_placement,log_device_placement=FLAGS.log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        cnn = QACNN(\n",
    "            _margin=FLAGS.margin,\n",
    "            sequence_length=FLAGS.sequence_length,\n",
    "            batch_size=FLAGS.batch_size,\n",
    "            vocab_size=len(vocab),\n",
    "            embedding_size=FLAGS.embedding_dim,\n",
    "            filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))),\n",
    "            num_filters=FLAGS.num_filters,\n",
    "            l2_reg_lambda=FLAGS.l2_reg_lambda)\n",
    "\n",
    "        # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-1)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        #define model saver\n",
    "        saver = tf.train.Saver(tf.global_variables(),max_to_keep=100)\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        def train_step(q, qp, qn):\n",
    "            feed_dict = {\n",
    "              cnn.q: q, cnn.qp: qp, cnn.qn: qn,\n",
    "              #cnn.input_x_1: q, cnn.input_x_2: qp, cnn.input_x_3: qn,\n",
    "              cnn.dropout_keep_prob: FLAGS.dropout_keep_prob\n",
    "            }\n",
    "            _,step, summaries, loss, accuracy, cos1, cos2 = sess.run(\n",
    "            [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy, cnn.cos_q_qp, cnn.cos_q_qn],feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "        def test_step(testList):\n",
    "            yp, y, group, of = [], [], [], open('../data/predict.txt', 'w')\n",
    "            start=0\n",
    "            end=start+FLAGS.batch_size\n",
    "            for i in range(0, len(testList), FLAGS.batch_size):\n",
    "                f, g, q1, q2 = gen_test_batch_qpn(testList,i,i+FLAGS.batch_size,vocab)\n",
    "                feed_dict = {\n",
    "                    cnn.q: q1, cnn.qp: q2, cnn.qn: q2,\n",
    "                    #cnn.input_x_1: q1, cnn.input_x_2: q2, cnn.input_x_3: q2,\n",
    "                    cnn.dropout_keep_prob: 1.0}\n",
    "                cos = sess.run([cnn.cos_q_qp], feed_dict)\n",
    "                yp.extend(cos[0])\n",
    "                y.extend(f)\n",
    "                group.extend(g)\n",
    "            y, g, yp = y[:len(testList)], group[:len(testList)], yp[:len(testList)]\n",
    "            auc = eval_auc(y[:len(testList)],yp[:len(testList)])\n",
    "            top1_prec,F1 = eval_top1_prec(y, g, yp)\n",
    "            for p in yp[:len(testList)]: \n",
    "                of.write(str(p) + '\\n')\n",
    "            of.write(str(top1_prec) + '\\n')\n",
    "            of.close()\n",
    "            return auc,top1_prec,F1\n",
    "\n",
    "        # Generate batches\n",
    "        # Training loop. For each batch...\n",
    "        #define average accuracy ,lowest and highest for accuracy and F1(recall)\n",
    "        aver=0\n",
    "        la=1\n",
    "        ha=0\n",
    "        lf=1\n",
    "        hf=0\n",
    "        #log id for best model\n",
    "        idx=0\n",
    "        for i in range(FLAGS.num_epochs):\n",
    "            try:\n",
    "                trainList=load_train_list()\n",
    "                #load shuffle testList \n",
    "                testList=load_test_list(trainList)\n",
    "                q, qp, qn = load_data(trainList,vocab, FLAGS.batch_size)\n",
    "                train_step(q, qp, qn)\n",
    "                current_step = tf.train.global_step(sess, global_step)\n",
    "                if current_step % FLAGS.evaluate_every == 0:\n",
    "                    auc,top1_prec,F1= test_step(testList)\n",
    "                    if la>top1_prec:\n",
    "                        la=top1_prec\n",
    "                    if lf>F1:\n",
    "                        lf=F1\n",
    "                    if ha<top1_prec:\n",
    "                        ha=top1_prec\n",
    "                        idx=current_step/FLAGS.checkpoint_every\n",
    "                    if hf<F1:\n",
    "                        hf=F1\n",
    "                    aver+=top1_prec\n",
    "                if current_step % FLAGS.checkpoint_every == 0:\n",
    "                    path = saver.save(sess, './model/model.ckpt', global_step=int(current_step/FLAGS.checkpoint_every))\n",
    "                    print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "        print(\"average top1_prec= {}\".format((aver*FLAGS.evaluate_every)/FLAGS.num_epochs))\n",
    "        print(\"lowest top1_prec={}\".format(la))\n",
    "        print(\"highest top1_prec={}\".format(ha))\n",
    "        print(\"id :{}\".format(idx))\n",
    "        print(\"lowest F1={}\".format(lf))\n",
    "        print(\"highest F1={}\".format(hf))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
